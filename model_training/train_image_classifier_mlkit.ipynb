{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "k0JGbIMvA8vP"
            },
            "source": [
                "# ðŸ“š Study Notes Image Classifier for ML Kit\n",
                "\n",
                "This notebook guides you through creating a custom TensorFlow Lite model that classifies images as either \"notes\" or \"others\". The model uses a 2-class categorical architecture with softmax activation and will be fully compatible with ML Kit's Custom Image Labeling on Android.\n",
                "\n",
                "## Model Architecture\n",
                "- **Base model**: MobileNetV2 pretrained on ImageNet\n",
                "- **Input shape**: (1, 224, 224, 3) â€” batch size 1, width & height 224, 3 RGB channels\n",
                "- **Output layer**: Dense layer with 2 units (for classes \"notes\" and \"others\") with softmax activation\n",
                "- **Output shape**: (1, 2) â€” tensor with class probabilities for the two classes\n",
                "\n",
                "## Overview\n",
                "\n",
                "1. Mount Google Drive & Install Dependencies\n",
                "2. Load and Preprocess Dataset\n",
                "3. Build & Train Model\n",
                "4. Convert to TFLite\n",
                "5. Add Required ML Kit Metadata\n",
                "6. Verify Model & Test Classification\n",
                "\n",
                "Let's get started!\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "YiNGgejUBRs_"
            },
            "source": [
                "## 1. Mount Google Drive & Install Dependencies\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "mLZw9YNhA1o8"
            },
            "outputs": [],
            "source": [
                "# Mount Google Drive to access the dataset\n",
                "from google.colab import drive\n",
                "\n",
                "drive.mount(\"/content/drive\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "tVAo5JvsBXIN"
            },
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import os\n",
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "import matplotlib.pyplot as plt\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                "from tensorflow.keras.applications import MobileNetV2, EfficientNetB0\n",
                "from tensorflow.keras import layers, models\n",
                "\n",
                "!python --version\n",
                "print(f\"TensorFlow version: {tf.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "G8l9vMVvBhxc"
            },
            "source": [
                "## 2. Load and Preprocess Dataset\n",
                "\n",
                "First, let's set up the path to our training data.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "j2L5NlHzBgqq"
            },
            "outputs": [],
            "source": [
                "# Set the path to your dataset in Google Drive\n",
                "# Update this path to match your Google Drive structure\n",
                "data_dir = \"/content/drive/MyDrive/training_data\"\n",
                "\n",
                "# Verify the directory structure\n",
                "print(f\"Classes: {os.listdir(data_dir)}\")\n",
                "print(f\"Number of 'notes' images: {len(os.listdir(os.path.join(data_dir, 'notes')))}\")\n",
                "print(f\"Number of 'others' images: {len(os.listdir(os.path.join(data_dir, 'others')))}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "K4XyeRZBB0Jv"
            },
            "source": [
                "Now, let's set up our data preprocessing and augmentation. This will help improve model performance and generalization.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "4S9JJMVPB6Wu"
            },
            "outputs": [],
            "source": [
                "# Model settings\n",
                "IMG_SIZE = 224  # Standard size for mobile models\n",
                "BATCH_SIZE = 32\n",
                "EPOCHS = 10\n",
                "\n",
                "# Data Augmentation for training\n",
                "train_datagen = ImageDataGenerator(\n",
                "    rescale=1.0 / 255,\n",
                "    validation_split=0.2,  # 20% for validation\n",
                "    rotation_range=20,\n",
                "    width_shift_range=0.2,\n",
                "    height_shift_range=0.2,\n",
                "    shear_range=0.2,\n",
                "    zoom_range=0.2,\n",
                "    horizontal_flip=True,\n",
                "    fill_mode=\"nearest\",\n",
                ")\n",
                "\n",
                "# Only rescaling for validation\n",
                "validation_datagen = ImageDataGenerator(\n",
                "    rescale=1.0 / 255,\n",
                "    validation_split=0.2,\n",
                ")\n",
                "\n",
                "# Create data generators\n",
                "train_generator = train_datagen.flow_from_directory(\n",
                "    data_dir,\n",
                "    target_size=(IMG_SIZE, IMG_SIZE),\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode=\"categorical\",  # categorical classification for 2 classes\n",
                "    subset=\"training\",\n",
                ")\n",
                "\n",
                "validation_generator = validation_datagen.flow_from_directory(\n",
                "    data_dir,\n",
                "    target_size=(IMG_SIZE, IMG_SIZE),\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode=\"categorical\",\n",
                "    subset=\"validation\",\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "14pnT4sJCHLK"
            },
            "source": [
                "Let's visualize some of our training images to check our data preprocessing:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "N9RcfHm7CCVc"
            },
            "outputs": [],
            "source": [
                "# Function to display images from our data generator\n",
                "def show_batch(image_batch, label_batch):\n",
                "    class_names = list(train_generator.class_indices.keys())\n",
                "    plt.figure(figsize=(10, 10))\n",
                "    for i in range(min(9, len(image_batch))):\n",
                "        ax = plt.subplot(3, 3, i + 1)\n",
                "        plt.imshow(image_batch[i])\n",
                "        label_idx = np.argmax(label_batch[i])\n",
                "        plt.title(class_names[label_idx])\n",
                "        plt.axis(\"off\")\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "# Get a batch of training images\n",
                "image_batch, label_batch = next(train_generator)\n",
                "show_batch(image_batch, label_batch)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "f_ug6x8QCWQF"
            },
            "source": [
                "## 3. Build & Train Model\n",
                "\n",
                "We'll use transfer learning with MobileNetV2, which is efficient and optimized for mobile applications.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "5LlZwEZDCblv"
            },
            "outputs": [],
            "source": [
                "# Create a model using MobileNetV2 as the base model\n",
                "def create_model():\n",
                "    # Base model: MobileNetV2 (optimized for mobile)\n",
                "    base_model = MobileNetV2(\n",
                "        weights=\"imagenet\",\n",
                "        include_top=False,\n",
                "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
                "    )\n",
                "\n",
                "    # Freeze the base model layers\n",
                "    base_model.trainable = False\n",
                "\n",
                "    # Create the model - architecture with 2 output units and softmax\n",
                "    model = models.Sequential(\n",
                "        [\n",
                "            base_model,\n",
                "            layers.GlobalAveragePooling2D(),\n",
                "            layers.BatchNormalization(),\n",
                "            layers.Dense(128, activation=\"relu\"),\n",
                "            layers.Dropout(0.5),\n",
                "            layers.Dense(2, activation=\"softmax\"),  # 2 units with softmax for \"notes\" and \"others\"\n",
                "        ]\n",
                "    )\n",
                "\n",
                "    # Compile the model\n",
                "    model.compile(\n",
                "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
                "        loss=\"categorical_crossentropy\",  # categorical crossentropy for 2-class softmax\n",
                "        metrics=[\"accuracy\"],\n",
                "    )\n",
                "\n",
                "    return model\n",
                "\n",
                "\n",
                "# Alternative model using EfficientNet\n",
                "def create_efficient_model():\n",
                "    base_model = EfficientNetB0(\n",
                "        weights=\"imagenet\",\n",
                "        include_top=False,\n",
                "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
                "    )\n",
                "\n",
                "    base_model.trainable = False\n",
                "\n",
                "    model = models.Sequential(\n",
                "        [\n",
                "            base_model,\n",
                "            layers.GlobalAveragePooling2D(),\n",
                "            layers.BatchNormalization(),\n",
                "            layers.Dense(128, activation=\"relu\"),\n",
                "            layers.Dropout(0.5),\n",
                "            layers.Dense(2, activation=\"softmax\"),  # 2 units with softmax\n",
                "        ]\n",
                "    )\n",
                "\n",
                "    model.compile(\n",
                "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
                "        loss=\"categorical_crossentropy\",\n",
                "        metrics=[\"accuracy\"],\n",
                "    )\n",
                "\n",
                "    return model\n",
                "\n",
                "\n",
                "# Create the model\n",
                "model = create_model()\n",
                "# model = create_efficient_model()\n",
                "\n",
                "# Display model summary\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "AuPP0bz_CiQs"
            },
            "outputs": [],
            "source": [
                "# Use callbacks for better training\n",
                "callbacks = [\n",
                "    tf.keras.callbacks.EarlyStopping(\n",
                "        monitor=\"val_loss\",\n",
                "        patience=3,\n",
                "        restore_best_weights=True,\n",
                "    ),\n",
                "    tf.keras.callbacks.ReduceLROnPlateau(\n",
                "        monitor=\"val_loss\",\n",
                "        factor=0.2,\n",
                "        patience=2,\n",
                "        min_lr=0.00001,\n",
                "    ),\n",
                "]\n",
                "\n",
                "# Train the model\n",
                "history = model.fit(\n",
                "    train_generator,\n",
                "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
                "    validation_data=validation_generator,\n",
                "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
                "    epochs=EPOCHS,\n",
                "    callbacks=callbacks,\n",
                "    verbose=1,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "xOKOLDqsCpLF"
            },
            "outputs": [],
            "source": [
                "# Plot training history\n",
                "acc = history.history[\"accuracy\"]\n",
                "val_acc = history.history[\"val_accuracy\"]\n",
                "loss = history.history[\"loss\"]\n",
                "val_loss = history.history[\"val_loss\"]\n",
                "\n",
                "plt.figure(figsize=(12, 4))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(acc, label=\"Training Accuracy\")\n",
                "plt.plot(val_acc, label=\"Validation Accuracy\")\n",
                "plt.xlabel(\"Epoch\")\n",
                "plt.ylabel(\"Accuracy\")\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(loss, label=\"Training Loss\")\n",
                "plt.plot(val_loss, label=\"Validation Loss\")\n",
                "plt.xlabel(\"Epoch\")\n",
                "plt.ylabel(\"Loss\")\n",
                "plt.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "lDJx5JvwC2X-"
            },
            "source": [
                "### Fine-tune the model\n",
                "\n",
                "Let's unfreeze some of the base model layers and continue training with a lower learning rate for better accuracy.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "background_save": true
                },
                "id": "ZHiO8JCLC7Mu"
            },
            "outputs": [],
            "source": [
                "# Fine-tuning\n",
                "def fine_tune_model():\n",
                "    # Unfreeze the top layers of the model\n",
                "    for layer in model.layers[0].layers[-30:]:  # Unfreeze last 30 layers of MobileNetV2\n",
                "        layer.trainable = True\n",
                "\n",
                "    # Recompile with a lower learning rate\n",
                "    model.compile(\n",
                "        optimizer=keras.optimizers.Adam(learning_rate=0.0001),  # Lower learning rate\n",
                "        loss=\"categorical_crossentropy\",  # categorical crossentropy for 2-class softmax\n",
                "        metrics=[\"accuracy\"],\n",
                "    )\n",
                "\n",
                "    # Train for a few more epochs\n",
                "    fine_tune_history = model.fit(\n",
                "        train_generator,\n",
                "        steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
                "        validation_data=validation_generator,\n",
                "        validation_steps=validation_generator.samples // BATCH_SIZE,\n",
                "        epochs=5,  # Just a few more epochs\n",
                "        callbacks=callbacks,\n",
                "        verbose=1,\n",
                "    )\n",
                "\n",
                "    return fine_tune_history\n",
                "\n",
                "\n",
                "# Run fine-tuning\n",
                "fine_tune_history = fine_tune_model()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "background_save": true
                },
                "id": "hfozGwbsa-or"
            },
            "outputs": [],
            "source": [
                "# Plot fine-tuning results\n",
                "ft_acc = fine_tune_history.history[\"accuracy\"]\n",
                "ft_val_acc = fine_tune_history.history[\"val_accuracy\"]\n",
                "ft_loss = fine_tune_history.history[\"loss\"]\n",
                "ft_val_loss = fine_tune_history.history[\"val_loss\"]\n",
                "\n",
                "plt.figure(figsize=(12, 4))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(ft_acc, label=\"Training Accuracy\")\n",
                "plt.plot(ft_val_acc, label=\"Validation Accuracy\")\n",
                "plt.xlabel(\"Epoch\")\n",
                "plt.ylabel(\"Accuracy\")\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(ft_loss, label=\"Training Loss\")\n",
                "plt.plot(ft_val_loss, label=\"Validation Loss\")\n",
                "plt.xlabel(\"Epoch\")\n",
                "plt.ylabel(\"Loss\")\n",
                "plt.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "fHKG3J9WDG8H"
            },
            "source": [
                "Now, let's save our trained model:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "background_save": true
                },
                "id": "YjDJTj3FDEoX"
            },
            "outputs": [],
            "source": [
                "# Create a directory for the SavedModel format\n",
                "!mkdir -p saved_model\n",
                "\n",
                "# Export the model\n",
                "model.export('saved_model/')\n",
                "print(\"Model saved in SavedModel format.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "o78aNbTgDSzv"
            },
            "source": [
                "## 4. Convert to TFLite\n",
                "\n",
                "Next, we'll convert our SavedModel to TFLite format for mobile deployment, with optimization for best performance.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "background_save": true
                },
                "id": "qNdPJQbpDSFG"
            },
            "outputs": [],
            "source": [
                "# Basic conversion to TFLite\n",
                "converter = tf.lite.TFLiteConverter.from_saved_model(\"saved_model/\")\n",
                "converter.target_spec.supported_ops = [\n",
                "    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TFLite ops\n",
                "    tf.lite.OpsSet.SELECT_TF_OPS     # enable TensorFlow ops\n",
                "]\n",
                "tflite_model = converter.convert()\n",
                "\n",
                "# Save the TFLite model\n",
                "with open(\"model.tflite\", \"wb\") as f:\n",
                "    f.write(tflite_model)\n",
                "\n",
                "print(f\"TFLite model size: {len(tflite_model) / (1024 * 1024):.2f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "mSwsScxmDcv3"
            },
            "outputs": [],
            "source": [
                "# Create a quantized model for better performance on mobile\n",
                "def create_quantized_model():\n",
                "    converter = tf.lite.TFLiteConverter.from_saved_model(\"saved_model/\")\n",
                "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "\n",
                "    # Add more quantization options for better mobile performance\n",
                "    converter.target_spec.supported_ops = [\n",
                "      tf.lite.OpsSet.TFLITE_BUILTINS_INT8,  # enable TFLite ops\n",
                "      tf.lite.OpsSet.SELECT_TF_OPS     # enable TensorFlow ops\n",
                "    ]\n",
                "\n",
                "    # Keep input and output as float for compatibility with ML Kit\n",
                "    converter.inference_input_type = tf.float32\n",
                "    converter.inference_output_type = tf.float32\n",
                "\n",
                "    # Representative dataset for calibration\n",
                "    def representative_dataset_gen():\n",
                "        # Use a few batches from the validation set for calibration\n",
                "        for _ in range(10):\n",
                "            img_batch, _ = next(validation_generator)\n",
                "            yield [img_batch]\n",
                "\n",
                "    converter.representative_dataset = representative_dataset_gen\n",
                "\n",
                "    quantized_tflite_model = converter.convert()\n",
                "\n",
                "    # Save the quantized model\n",
                "    with open(\"model_quantized.tflite\", \"wb\") as f:\n",
                "        f.write(quantized_tflite_model)\n",
                "\n",
                "    print(f\"Quantized TFLite model size: {len(quantized_tflite_model) / (1024 * 1024):.2f} MB\")\n",
                "    return quantized_tflite_model\n",
                "\n",
                "\n",
                "# Create a quantized model for better mobile performance\n",
                "quantized_tflite_model = create_quantized_model()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "5-j-OMZiDlRH"
            },
            "source": [
                "## 5. Add Required ML Kit Metadata\n",
                "\n",
                "Now we need to add the required metadata to make our model compatible with ML Kit's Custom Image Labeling.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "mHXIhBW_Dg6A"
            },
            "outputs": [],
            "source": [
                "# Create a labels.txt file with our class names\n",
                "class_indices = train_generator.class_indices\n",
                "class_labels = {v: k for k, v in class_indices.items()}\n",
                "\n",
                "with open('labels.txt', 'w') as f:\n",
                "    for i in range(len(class_labels)):\n",
                "        f.write(f\"{class_labels[i]}\\n\")\n",
                "\n",
                "print(\"Labels file created:\")\n",
                "!cat labels.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 635
                },
                "id": "7ne8KfSylWTf",
                "outputId": "ba04d483-1a09-42ac-f5c9-f70174d067f6"
            },
            "outputs": [],
            "source": [
                "%pip install tflite-support"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 339
                },
                "id": "KULEeejMDtHG",
                "outputId": "2c7cdc4d-8b7a-4c2d-e72f-71c3ff6f88c2"
            },
            "outputs": [],
            "source": [
                "# Add metadata to the TFLite model\n",
                "from tensorflow_lite_support.metadata.python import metadata\n",
                "from tensorflow_lite_support.metadata import metadata_schema_py_generated as _metadata_fb\n",
                "import flatbuffers\n",
                "import os\n",
                "\n",
                "\n",
                "# Function to add metadata to a model\n",
                "def add_metadata(model_path, output_path):\n",
                "    \"\"\"Creates the metadata for a study notes image classifier.\"\"\"\n",
                "\n",
                "    # Creates model info.\n",
                "    model_meta = _metadata_fb.ModelMetadataT()\n",
                "    model_meta.name = \"Study Notes Classifier\"\n",
                "    model_meta.description = (\n",
                "        \"Categorical classifier that identifies whether an image contains \"\n",
                "        \"study notes or other content. The model outputs probabilities \"\n",
                "        \"for two classes: 'notes' and 'others' using softmax activation.\"\n",
                "    )\n",
                "    model_meta.version = \"v1.0\"\n",
                "    model_meta.author = \"Bit-Blazer\"\n",
                "    model_meta.license = (\n",
                "        \"MIT License. \"\n",
                "        \"https://opensource.org/licenses/MIT\"\n",
                "    )\n",
                "\n",
                "    # Creates input info.\n",
                "    input_meta = _metadata_fb.TensorMetadataT()\n",
                "    input_meta.name = \"image\"\n",
                "    input_meta.description = (\n",
                "        \"Input image to be classified as 'notes' or 'others'. \"\n",
                "        \"The expected image is {0} x {1}, with three channels (red, green, blue) \"\n",
                "        \"per pixel. Each value in the tensor is normalized to the range [0, 1] \"\n",
                "        \"using pixel_value / 255.0.\".format(224, 224)\n",
                "    )\n",
                "\n",
                "    input_meta.content = _metadata_fb.ContentT()\n",
                "    input_meta.content.contentProperties = _metadata_fb.ImagePropertiesT()\n",
                "    input_meta.content.contentProperties.colorSpace = _metadata_fb.ColorSpaceType.RGB\n",
                "    input_meta.content.contentPropertiesType = (\n",
                "        _metadata_fb.ContentProperties.ImageProperties\n",
                "    )\n",
                "\n",
                "    # Add normalization options for input preprocessing\n",
                "    input_normalization = _metadata_fb.ProcessUnitT()\n",
                "    input_normalization.optionsType = (\n",
                "        _metadata_fb.ProcessUnitOptions.NormalizationOptions\n",
                "    )\n",
                "    input_normalization.options = _metadata_fb.NormalizationOptionsT()\n",
                "    input_normalization.options.mean = [0.0]\n",
                "    input_normalization.options.std = [255.0]\n",
                "    input_meta.processUnits = [input_normalization]\n",
                "\n",
                "    # Add input statistics\n",
                "    input_stats = _metadata_fb.StatsT()\n",
                "    input_stats.max = [255.0]\n",
                "    input_stats.min = [0.0]\n",
                "    input_meta.stats = input_stats\n",
                "\n",
                "    # Creates output info.\n",
                "    output_meta = _metadata_fb.TensorMetadataT()\n",
                "    output_meta.name = \"probability\"\n",
                "    output_meta.description = (\n",
                "        \"Output probabilities for the two classes: 'notes' and 'others'. \"\n",
                "        \"The tensor contains 2 values representing the softmax probabilities \"\n",
                "        \"for each class. Index 0 corresponds to 'notes' class, \"\n",
                "        \"index 1 corresponds to 'others' class.\"\n",
                "    )\n",
                "\n",
                "    output_meta.content = _metadata_fb.ContentT()\n",
                "    output_meta.content.contentProperties = _metadata_fb.FeaturePropertiesT()\n",
                "    output_meta.content.contentPropertiesType = (\n",
                "        _metadata_fb.ContentProperties.FeatureProperties\n",
                "    )\n",
                "\n",
                "    # Add score thresholding options (recommended by ML Kit docs)\n",
                "    score_thresholding = _metadata_fb.ProcessUnitT()\n",
                "    score_thresholding.optionsType = (\n",
                "        _metadata_fb.ProcessUnitOptions.ScoreThresholdingOptions\n",
                "    )\n",
                "    score_thresholding.options = _metadata_fb.ScoreThresholdingOptionsT()\n",
                "    score_thresholding.options.globalScoreThreshold = 0.5  # Default threshold for classification\n",
                "    output_meta.processUnits = [score_thresholding]\n",
                "\n",
                "    # Add output statistics\n",
                "    output_stats = _metadata_fb.StatsT()\n",
                "    output_stats.max = [1.0]\n",
                "    output_stats.min = [0.0]\n",
                "    output_meta.stats = output_stats\n",
                "\n",
                "    # Add label file for ML Kit compatibility\n",
                "    label_file = _metadata_fb.AssociatedFileT()\n",
                "    label_file.name = os.path.basename(\"labels.txt\")\n",
                "    label_file.description = \"Labels for study notes classification: notes and others.\"\n",
                "    label_file.type = _metadata_fb.AssociatedFileType.TENSOR_AXIS_LABELS\n",
                "    output_meta.associatedFiles = [label_file]\n",
                "\n",
                "    # Creates subgraph info.\n",
                "    subgraph = _metadata_fb.SubGraphMetadataT()\n",
                "    subgraph.inputTensorMetadata = [input_meta]\n",
                "    subgraph.outputTensorMetadata = [output_meta]\n",
                "    model_meta.subgraphMetadata = [subgraph]\n",
                "\n",
                "    # Build and populate metadata\n",
                "    b = flatbuffers.Builder(0)\n",
                "    b.Finish(\n",
                "        model_meta.Pack(b),\n",
                "        metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER,\n",
                "    )\n",
                "    metadata_buf = b.Output()\n",
                "\n",
                "    # Populate the model with metadata\n",
                "    populator = metadata.MetadataPopulator.with_model_file(model_path)\n",
                "    populator.load_metadata_buffer(metadata_buf)\n",
                "    populator.load_associated_files([\"labels.txt\"])\n",
                "    populator.populate()\n",
                "\n",
                "    # Save the model with metadata to output path\n",
                "    with open(model_path, \"rb\") as f:\n",
                "        model_with_metadata = f.read()\n",
                "\n",
                "    with open(output_path, \"wb\") as f:\n",
                "        f.write(model_with_metadata)\n",
                "\n",
                "    print(f\"Model with detailed metadata saved to {output_path}\")\n",
                "    print(f\"Metadata includes:\")\n",
                "    print(f\"  - Model: {model_meta.name} v{model_meta.version}\")\n",
                "    print(f\"  - Author: {model_meta.author}\")\n",
                "    print(f\"  - License: MIT\")\n",
                "    print(f\"  - Input: 224x224 RGB image with normalization [0,1]\")\n",
                "    print(f\"  - Output: 2-class softmax probabilities for 'notes' and 'others'\")\n",
                "    print(f\"  - Score Threshold: {score_thresholding.options.globalScoreThreshold} (recommended by ML Kit)\")\n",
                "    print(f\"  - Labels: {label_file.name} attached\")\n",
                "\n",
                "    return model_with_metadata\n",
                "\n",
                "\n",
                "# Add metadata to both models\n",
                "model_with_metadata = add_metadata(\"model.tflite\", \"model_with_metadata.tflite\")\n",
                "quantized_model_with_metadata = add_metadata(\"model_quantized.tflite\", \"model_quantized_with_metadata.tflite\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "YGgE1e8PD6UP"
            },
            "source": [
                "## 6. Verify Model & Test Classification\n",
                "\n",
                "Let's test our model on some sample images to make sure it's working correctly.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "NLWWDw5wD1Jw"
            },
            "outputs": [],
            "source": [
                "# Function to load and preprocess an image for prediction\n",
                "def preprocess_image(image_path):\n",
                "    img = tf.keras.preprocessing.image.load_img(\n",
                "        image_path, target_size=(IMG_SIZE, IMG_SIZE)\n",
                "    )\n",
                "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
                "    img_array = img_array / 255.0  # Normalize to [0,1]\n",
                "    img_array = np.expand_dims(img_array, axis=0)  # Create batch dimension\n",
                "    return img_array, img\n",
                "\n",
                "\n",
                "# Function to make predictions using the SavedModel\n",
                "def predict_with_model(image_path):\n",
                "    img_array, img = preprocess_image(image_path)\n",
                "    predictions = model.predict(img_array)\n",
                "\n",
                "    # Get class indices mapping\n",
                "    class_indices = train_generator.class_indices\n",
                "    class_labels = {v: k for k, v in class_indices.items()}\n",
                "    \n",
                "    probabilities = predictions[0]\n",
                "    predicted_class_idx = np.argmax(probabilities)\n",
                "    predicted_class = class_labels[predicted_class_idx]\n",
                "    confidence = float(probabilities[predicted_class_idx])\n",
                "\n",
                "    plt.figure(figsize=(6, 6))\n",
                "    plt.imshow(img)\n",
                "    plt.title(f\"Prediction: {predicted_class} (Confidence: {confidence:.4f})\")\n",
                "    plt.axis(\"off\")\n",
                "    plt.show()\n",
                "\n",
                "    # Display probabilities with correct class names\n",
                "    prob_str = \", \".join([f\"{class_labels[i]}={probabilities[i]:.4f}\" for i in range(len(class_labels))])\n",
                "    print(f\"Class probabilities: {prob_str}\")\n",
                "    \n",
                "    return predicted_class, confidence"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "background_save": true
                },
                "id": "ZR69jtXYEJC2"
            },
            "outputs": [],
            "source": [
                "# Test on sample images from the validation set\n",
                "def test_on_validation_images():\n",
                "    # Test on notes images\n",
                "    test_dir = os.path.join(data_dir, \"notes\")\n",
                "    test_images = os.listdir(test_dir)[:5]  # Get first 5 images\n",
                "\n",
                "    print(\"\\n--- Testing on 'notes' images ---\")\n",
                "    for image_name in test_images:\n",
                "        image_path = os.path.join(test_dir, image_name)\n",
                "        print(f\"Testing image: {image_name}\")\n",
                "        predicted_class, confidence = predict_with_model(image_path)\n",
                "        print(f\"Predicted class: {predicted_class}, Confidence: {confidence:.4f}\")\n",
                "\n",
                "    # Test on others images\n",
                "    test_dir = os.path.join(data_dir, \"others\")\n",
                "    test_images = os.listdir(test_dir)[:5]  # Get first 5 images\n",
                "\n",
                "    print(\"\\n--- Testing on 'others' images ---\")\n",
                "    for image_name in test_images:\n",
                "        image_path = os.path.join(test_dir, image_name)\n",
                "        print(f\"Testing image: {image_name}\")\n",
                "        predicted_class, confidence = predict_with_model(image_path)\n",
                "        print(f\"Predicted class: {predicted_class}, Confidence: {confidence:.4f}\")\n",
                "\n",
                "\n",
                "# Run tests on validation images\n",
                "test_on_validation_images()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "jFxeP7JTEapW"
            },
            "source": [
                "## Test the TFLite Model\n",
                "\n",
                "Now let's test our TFLite model with metadata to make sure it's working correctly.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "background_save": true
                },
                "id": "Wz663ZD1MJtK"
            },
            "outputs": [],
            "source": [
                "# Function to test the TFLite model\n",
                "def test_tflite_model(tflite_model_path, image_path):\n",
                "    # Load the TFLite model\n",
                "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
                "    interpreter.allocate_tensors()\n",
                "\n",
                "    # Get input and output details\n",
                "    input_details = interpreter.get_input_details()\n",
                "    output_details = interpreter.get_output_details()\n",
                "\n",
                "    # Check the type of the input tensor\n",
                "    floating_model = input_details[0][\"dtype\"] == np.float32\n",
                "\n",
                "    # Get input shape - NxHxWxC, H:1, W:2\n",
                "    height = input_details[0][\"shape\"][1]\n",
                "    width = input_details[0][\"shape\"][2]\n",
                "\n",
                "    # Load an image\n",
                "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(height, width))\n",
                "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
                "\n",
                "    # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n",
                "    if floating_model:\n",
                "        img_array = img_array / 255.0  # Normalize to [0,1] as per our training\n",
                "\n",
                "    # Add batch dimension\n",
                "    input_data = np.expand_dims(img_array, axis=0).astype(input_details[0][\"dtype\"])\n",
                "\n",
                "    # Set the tensor to point to the input data to be inferred\n",
                "    interpreter.set_tensor(input_details[0][\"index\"], input_data)\n",
                "\n",
                "    # Run the inference\n",
                "    interpreter.invoke()\n",
                "\n",
                "    # Extract the output\n",
                "    output_data = interpreter.get_tensor(output_details[0][\"index\"])\n",
                "\n",
                "    # Process the result - 2-class softmax output\n",
                "    class_indices = train_generator.class_indices\n",
                "    class_labels = {v: k for k, v in class_indices.items()}\n",
                "\n",
                "    \n",
                "    probabilities = output_data[0]\n",
                "    predicted_class_idx = np.argmax(probabilities)\n",
                "    predicted_class = class_labels[predicted_class_idx]\n",
                "    confidence = float(probabilities[predicted_class_idx])\n",
                "\n",
                "    print(f\"  Result: {predicted_class} with confidence {confidence:.4f}\")\n",
                "    prob_str = \", \".join([f\"{class_labels[i]}={probabilities[i]:.4f}\" for i in range(len(class_labels))])\n",
                "    print(f\"  Class probabilities: {prob_str}\")\n",
                "    return predicted_class, confidence\n",
                "\n",
                "\n",
                "# Test the TFLite models\n",
                "def test_tflite_models():\n",
                "    # Test normal TFLite model\n",
                "    print(\"\\n--- Testing TFLite model with metadata ---\")\n",
                "    tflite_model_path = \"model_with_metadata.tflite\"\n",
                "\n",
                "    # Test on a few notes images\n",
                "    test_dir = os.path.join(data_dir, \"notes\")\n",
                "    test_images = os.listdir(test_dir)[:3]\n",
                "\n",
                "    print(\"Testing on 'notes' images:\")\n",
                "    for image_name in test_images:\n",
                "        image_path = os.path.join(test_dir, image_name)\n",
                "        print(f\"Image: {image_name}\")\n",
                "        test_tflite_model(tflite_model_path, image_path)\n",
                "\n",
                "    # Test on a few others images\n",
                "    test_dir = os.path.join(data_dir, \"others\")\n",
                "    test_images = os.listdir(test_dir)[:3]\n",
                "\n",
                "    print(\"\\nTesting on 'others' images:\")\n",
                "    for image_name in test_images:\n",
                "        image_path = os.path.join(test_dir, image_name)\n",
                "        print(f\"Image: {image_name}\")\n",
                "        test_tflite_model(tflite_model_path, image_path)\n",
                "\n",
                "    # Test quantized model\n",
                "    print(\"\\n--- Testing Quantized TFLite model with metadata ---\")\n",
                "    quantized_model_path = \"model_quantized_with_metadata.tflite\"\n",
                "\n",
                "    # Test on a few notes images\n",
                "    test_dir = os.path.join(data_dir, \"notes\")\n",
                "    test_images = os.listdir(test_dir)[:3]\n",
                "\n",
                "    print(\"Testing on 'notes' images:\")\n",
                "    for image_name in test_images:\n",
                "        image_path = os.path.join(test_dir, image_name)\n",
                "        print(f\"Image: {image_name}\")\n",
                "        test_tflite_model(quantized_model_path, image_path)\n",
                "\n",
                "    # Test on a few others images\n",
                "    test_dir = os.path.join(data_dir, \"others\")\n",
                "    test_images = os.listdir(test_dir)[:3]\n",
                "\n",
                "    print(\"\\nTesting on 'others' images:\")\n",
                "    for image_name in test_images:\n",
                "        image_path = os.path.join(test_dir, image_name)\n",
                "        print(f\"Image: {image_name}\")\n",
                "        test_tflite_model(quantized_model_path, image_path)\n",
                "\n",
                "\n",
                "# Run tests on TFLite models\n",
                "test_tflite_models()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "t_jhvW78FjkZ"
            },
            "source": [
                "## 7. Examine Model Metadata\n",
                "\n",
                "Let's verify that our model has all the required metadata for ML Kit.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "background_save": true
                },
                "id": "Fuh5Y7w0FijV"
            },
            "outputs": [],
            "source": [
                "# Display Metadata\n",
                "# from tflite_support import metadata\n",
                "\n",
                "from tensorflow_lite_support.metadata.python import metadata\n",
                "\n",
                "\n",
                "# Function to display metadata of a model\n",
                "def display_metadata(model_path):\n",
                "    print(f\"\\n--- Metadata for {model_path} ---\")\n",
                "\n",
                "    # Print metadata\n",
                "    displayer = metadata.MetadataDisplayer.with_model_file(model_path)\n",
                "    metadata_json = displayer.get_metadata_json()\n",
                "    print(\"Metadata JSON:\")\n",
                "    import json\n",
                "\n",
                "    print(json.dumps(json.loads(metadata_json), indent=2))\n",
                "\n",
                "    # Print associated files\n",
                "    associated_files = displayer.get_packed_associated_file_list()\n",
                "    print(\"\\nAssociated files:\")\n",
                "    for file in associated_files:\n",
                "        print(file)\n",
                "\n",
                "    # Get label file content\n",
                "    label_file_content = displayer.get_associated_file_buffer(\"labels.txt\")\n",
                "    print(\"\\nLabels file content:\")\n",
                "    print(label_file_content.decode(\"utf-8\"))\n",
                "\n",
                "\n",
                "# Display metadata for both models\n",
                "display_metadata(\"model_with_metadata.tflite\")\n",
                "display_metadata(\"model_quantized_with_metadata.tflite\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "YzEqWqK5F0Ih"
            },
            "source": [
                "## 8. Save Models for Android ML Kit Integration\n",
                "\n",
                "Let's save our TFLite models with metadata to use in our Android app with ML Kit.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "background_save": true
                },
                "id": "rrjdRgJcF5-B"
            },
            "outputs": [],
            "source": [
                "# Save the models to Google Drive\n",
                "output_dir = '/content/drive/MyDrive/ml_kit_models'\n",
                "!mkdir -p $output_dir\n",
                "\n",
                "# Copy the model files\n",
                "!cp model_with_metadata.tflite $output_dir/notes_classifier.tflite\n",
                "!cp model_quantized_with_metadata.tflite $output_dir/notes_classifier_quantized.tflite\n",
                "\n",
                "# Also save the labels file separately (for reference)\n",
                "!cp labels.txt $output_dir/labels.txt\n",
                "\n",
                "print(f\"Models saved to {output_dir}/\")\n",
                "print(f\"- Standard model: notes_classifier.tflite\")\n",
                "print(f\"- Quantized model: notes_classifier_quantized.tflite\")\n",
                "print(f\"- Labels file: labels.txt\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "lJLjMHH-GxLj"
            },
            "source": [
                "## Troubleshooting & Optimization Tips\n",
                "\n",
                "### Common Issues:\n",
                "\n",
                "1. **Poor Accuracy**:\n",
                "   - Try collecting more training data\n",
                "   - Use data augmentation (already implemented in this notebook)\n",
                "   - Try different base models (MobileNetV2, EfficientNet)\n",
                "   - Fine-tune more layers of the base model\n",
                "2. **Model Size Issues**:\n",
                "\n",
                "   - Use quantization (already shown in this notebook)\n",
                "   - Try a smaller base model (MobileNetV2 instead of EfficientNet)\n",
                "   - Use pruning techniques to remove unnecessary weights\n",
                "\n",
                "3. **ML Kit Integration**:\n",
                "   - Make sure the model has proper metadata\n",
                "   - Verify model input/output tensors match ML Kit requirements\n",
                "   - Check Android logs for specific error messages\n",
                "\n",
                "### Optimizing Performance:\n",
                "\n",
                "1. **Preprocessing**:\n",
                "   - Ensure consistent preprocessing between training and inference\n",
                "   - Consider adding normalization parameters to metadata\n",
                "2. **Hardware Acceleration**:\n",
                "\n",
                "   - ML Kit uses hardware acceleration when available\n",
                "   - Can specify GPU/NNAPI delegates for specific use cases\n",
                "\n",
                "3. **Further Model Optimization**:\n",
                "   - Try dynamic range quantization or float16 quantization\n",
                "   - Use TensorFlow Model Optimization Toolkit for advanced techniques\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ewA4RrHtG-qj"
            },
            "source": [
                "## Conclusion\n",
                "\n",
                "You now have a fully functional TensorFlow Lite model with the specified architecture that can classify images using a 2-class categorical approach, ready for integration with ML Kit in your Android app. \n",
                "\n",
                "### Model Architecture Summary:\n",
                "- **Base model**: MobileNetV2 pretrained on ImageNet\n",
                "- **Input shape**: (1, 224, 224, 3) â€” batch size 1, width & height 224, 3 RGB channels\n",
                "- **Output layer**: Dense layer with 2 units and softmax activation\n",
                "- **Output shape**: (1, 2) â€” tensor with class probabilities for \"notes\" and \"others\"\n",
                "- **Classes**: Determined by labels.txt file, mapped from folder structure\n",
                "\n",
                "The model follows all the required specifications for ML Kit Custom Image Labeling, including proper input/output tensors and metadata.\n",
                "\n",
                "This notebook has covered:\n",
                "\n",
                "- Loading and preprocessing your dataset from Google Drive\n",
                "- Building and training a categorical classifier using transfer learning\n",
                "- Converting the model to TFLite format with proper 2-class softmax output\n",
                "- Adding the required metadata for ML Kit compatibility\n",
                "- Testing the model on sample images with correct class probability interpretation\n",
                "\n",
                "The final model is saved in your Google Drive at `/content/drive/MyDrive/ml_kit_models/notes_classifier.tflite` and is ready to be used in your Android app!\n"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "provenance": []
        },
        "gpuClass": "standard",
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
